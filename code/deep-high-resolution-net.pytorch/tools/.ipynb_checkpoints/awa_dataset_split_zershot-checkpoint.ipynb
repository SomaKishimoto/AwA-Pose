{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "import shutil\n",
    "import pickle\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "import _init_paths\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from core.loss import JointsMSELoss\n",
    "from core.function import train\n",
    "from core.function import validate\n",
    "from utils.utils import get_optimizer\n",
    "from utils.utils import save_checkpoint\n",
    "from utils.utils import create_logger\n",
    "from utils.utils import get_model_summary\n",
    "\n",
    "import dataset\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['annotations', 'images', 'categories', 'info'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/u/snaha/v6/dataset/AWA/Animals_with_Attributes2/quadruped_keypoints/coco_format/dataset.pickle', 'rb') as handle:\n",
    "    dataset = pickle.load(handle)\n",
    "    \n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 1024,\n",
       " 'height': 768,\n",
       " 'filename': 'antelope_10002.jpg',\n",
       " 'id': 'antelope_10002'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antelope',\n",
       " 'bobcat',\n",
       " 'buffalo',\n",
       " 'chihuahua',\n",
       " 'collie',\n",
       " 'cow',\n",
       " 'dalmatian',\n",
       " 'deer',\n",
       " 'elephant',\n",
       " 'fox',\n",
       " 'german+shepherd',\n",
       " 'giant+panda',\n",
       " 'giraffe',\n",
       " 'grizzly+bear',\n",
       " 'hippopotamus',\n",
       " 'horse',\n",
       " 'leopard',\n",
       " 'lion',\n",
       " 'moose',\n",
       " 'otter',\n",
       " 'ox',\n",
       " 'persian+cat',\n",
       " 'pig',\n",
       " 'polar+bear',\n",
       " 'rabbit',\n",
       " 'raccoon',\n",
       " 'rhinoceros',\n",
       " 'sheep',\n",
       " 'siamese+cat',\n",
       " 'skunk',\n",
       " 'squirrel',\n",
       " 'tiger',\n",
       " 'weasel',\n",
       " 'wolf',\n",
       " 'zebra']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/u/snaha/v6/dataset/AWA/Animals_with_Attributes2/quadruped_keypoints/Annotations/'\n",
    "list_subfolders_with_paths = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "\n",
    "animal_list = []\n",
    "\n",
    "for animal in list_subfolders_with_paths:\n",
    "    animal_folder = animal#os.path.join(path, animal)\n",
    "    animal_list.append(animal_folder.split('/')[-1])\n",
    "    \n",
    "animal_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train instances =  9439 9439\n",
      "Number of test instances =  613 613\n",
      "Number of train instances =  9361 9361\n",
      "Number of test instances =  691 691\n",
      "Number of train instances =  9376 9376\n",
      "Number of test instances =  676 676\n",
      "Number of train instances =  9473 9473\n",
      "Number of test instances =  579 579\n",
      "Number of train instances =  9451 9451\n",
      "Number of test instances =  601 601\n"
     ]
    }
   ],
   "source": [
    "def generate_split(animal_list, dataset, all_full_animal_image_list_dict):\n",
    "    \n",
    "    rand_inds = np.arange(len(animal_list)).tolist()\n",
    "    random.shuffle(rand_inds)    \n",
    "    anim_list = []\n",
    "    for ind in rand_inds:\n",
    "        anim_list.append(animal_list[ind])\n",
    "    \n",
    "    import ipdb; ipdb.set_trace()\n",
    "    exit(0) \n",
    "    dataset_train = dict()\n",
    "    dataset_test = dict() \n",
    "    \n",
    "    annotations_train = []\n",
    "    annotations_test = []\n",
    "    \n",
    "    images_train = []\n",
    "    images_test = []\n",
    "    \n",
    "    train_animals = anim_list[:-4]\n",
    "    test_animal = anim_list[-4:]\n",
    "    \n",
    "    for im, anno in zip(dataset['images'], dataset['annotations']):\n",
    "        \n",
    "#         import ipdb; ipdb.set_trace()\n",
    "#         exit(0)         \n",
    "        if anno['animal'] in test_animal and im['filename'].split('_')[0] in test_animal \\\n",
    "        and im['filename'].split('.')[0] in all_full_animal_image_list_dict[anno['animal']]:\n",
    "            annotations_test.append(anno)\n",
    "            images_test.append(im)\n",
    "        else:\n",
    "            annotations_train.append(anno)\n",
    "            images_train.append(im)\n",
    "        \n",
    "#     for anno in dataset['images']:\n",
    "#         if anno['id'].split('_')[0] in test_animal:\n",
    "#             images_test.append(anno)\n",
    "#         else:\n",
    "#             images_train.append(anno)        \n",
    "                \n",
    "    dataset_train['annotations'] = annotations_train\n",
    "    dataset_test['annotations'] = annotations_test\n",
    "    \n",
    "    dataset_train['images'] = images_train\n",
    "    dataset_test['images'] = images_test\n",
    "    \n",
    "    dataset_train['categories'] = dataset['categories']\n",
    "    dataset_test['categories'] = dataset['categories']    \n",
    "    \n",
    "    dataset_train['info'] = dataset['info']\n",
    "    dataset_test['info'] = dataset['info']       \n",
    "    \n",
    "    return train_animals, test_animal, dataset_train, dataset_test\n",
    "\n",
    "#all_full_animal_image_list_dict = \n",
    "with open('/u/snaha/v6/dataset/AWA/Animals_with_Attributes2/quadruped_keypoints/Annotations/all_full_animal_image_list_dict.pickle', 'rb') as handle:\n",
    "    all_full_animal_image_list_dict = pickle.load(handle)\n",
    "    \n",
    "for i in range(5):\n",
    "    train_animals, test_animals, dataset_train, dataset_test = generate_split(animal_list, dataset, all_full_animal_image_list_dict)\n",
    "    #test_animals\n",
    "    print('Number of train instances = ', len(dataset_train['images']), len(dataset_train['annotations']))\n",
    "    print('Number of test instances = ', len(dataset_test['images']), len(dataset_test['annotations']))\n",
    "    \n",
    "    with open('/u/snaha/v6/dataset/AWA/Animals_with_Attributes2/quadruped_keypoints/coco_format/dataset_train_' + str(i+1) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(dataset_train, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "        \n",
    "    with open('/u/snaha/v6/dataset/AWA/Animals_with_Attributes2/quadruped_keypoints/coco_format/dataset_val_' + str(i+1) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(dataset_test, handle, protocol=pickle.HIGHEST_PROTOCOL)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [.24, .72, .72, .18, .62, .23, .56, .26, .38, .28, .29, .48, .23, .60, .47, .24, .72, .72, .18, .62, .23, .56, .26, .38, .28, .29, .48, .23, .60, .47, .18, .62, .23, .56, .26, .38, .28, .29, .48]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
